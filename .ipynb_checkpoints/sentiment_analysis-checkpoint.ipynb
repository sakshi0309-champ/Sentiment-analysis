{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Sentiment Analysis</H1>\n",
    "<br>\n",
    "Internet is reaching to more and more people everyday. There is more and more interaction among people through social networking sites. While we have plenty of positives, we can't deny existence of onlune bullying. Sentiment analysis can help us tackle this. Here's an attempt at doing that using scikit, nltk and panda.\n",
    "\n",
    "Panda helps us read csv data. There will be a seperate post on pandas.\n",
    "\n",
    "Panda read the training data and presented the data to us in the form of data frame. You may think data frame as sql table from where we can query data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1231.0</td>\n",
       "      <td>you are bad.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  comment_text  toxic\n",
       "0  1231.0  you are bad.      1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"../../..//Downloads/train_1.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "It's very important to split the given data into two parts. One for training our model and other for testing our model. Scikit provides us with an option to split the data with <i>train_test_split</i>. <i>random_state</i> ensures same data is there in test and train, how much ever times you split.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.comment_text, df.toxic, test_size=0.20, random_state=42)\n",
    "name = df.toxic.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Let's be practical. Any data in the world will not be ideal with correct spelling. Some of them will also have internet typo like itttttttt for it. Textblob provides us with collection of words. <i>TextBlob(message).words</i> will give us collection of words from a sentence. We can do word.correct() to correct spelling of the word. It has about 70% accuracy. Lemma is converting words into it's root form. Like given <i>playing</i> it will return <i>play</i>. word.lemmatize() is a callable function to do the same. \n",
    "\n",
    "It's important to preprocess because any model will ultimately rely on the occurence of similar words in test and train data. \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import nltk\n",
    "def split_into_lemmas(message):\n",
    "    message = unicode(message, 'utf8').lower()\n",
    "    words = TextBlob(message).words\n",
    "    return [word.lemma for word in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We are going to train four different learning models on the data given. We are going to compare the results of the\n",
    "different classifiers. Describing each classifier is beyond the scope of this post. I will provide the relevent link \n",
    "at the end of this post.\n",
    "\n",
    "Pipelines are like a collection of function which is applied to text data sequentially.\n",
    "\n",
    "Here our pipeline has three functions:\n",
    "    a. CountVectorizer()\n",
    "    b. TfidfTransformer()\n",
    "    c. Classifier() which is SGDClassifier, LogisticRegression, MultinomialNB, SVC in our four different pipeline\n",
    "    \n",
    "CountVectorizer : Convert a collection of text documents to a matrix of token counts. For example : \"You are awesome\"\n",
    "        will be returned as per the analyzer given to CountVectorizer which is split_into_lemmas in our case. So, the \n",
    "        CountVectorizer will turn \"You are awesome\" into \"You\", \"are\", \"awesome\".  ngram_range further split each token\n",
    "        into substrings. We have given ngram_range as (2,4), which means each word will be substringed into substring\n",
    "        of 2, 3 and 4 characters. stop_words remove the commonly occuring english words from the given text.\n",
    "\n",
    "TfidfTransformer : Transform a count matrix to a normalized tf or tf-idf representation. Tf means term-frequency \n",
    "    while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme \n",
    "    in information retrieval, that has also found good use in document classification. The goal of using \n",
    "    tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down \n",
    "    the impact of tokens that occur very frequently in a given corpus and that are hence empirically less \n",
    "    informative than features that occur in a small fraction of the training corpus.\n",
    "    \n",
    "Classifier : Classifier actually learns the data and classifies into the labels. \n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>Making pipeline of SGDClassifier</h2>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer=<function split_into_lemmas at 0x11bd320c8>,\n",
       "        binary=False, decode_error=u'strict', dtype=<type 'numpy.int64'>,\n",
       "        encoding=u'utf-8', input=u'content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(2, 4), preprocess...   penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf_SGDClassifier = Pipeline([('vect', CountVectorizer(analyzer=split_into_lemmas, ngram_range=(2,4), stop_words='english',lowercase=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier()),\n",
    "])\n",
    "text_clf_SGDClassifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>Making pipeline of LogisticRegression<h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer=<function split_into_lemmas at 0x11bd320c8>,\n",
       "        binary=False, decode_error=u'strict', dtype=<type 'numpy.int64'>,\n",
       "        encoding=u'utf-8', input=u'content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(2, 4), preprocess...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_LogisticRegression = Pipeline([('vect', CountVectorizer(analyzer=split_into_lemmas, ngram_range=(2,4), stop_words='english',lowercase=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression()),\n",
    "])\n",
    "text_clf_LogisticRegression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>Making pipeline of MultinomialNB<h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer=<function split_into_lemmas at 0x11bd320c8>,\n",
       "        binary=False, decode_error=u'strict', dtype=<type 'numpy.int64'>,\n",
       "        encoding=u'utf-8', input=u'content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(2, 4), preprocess...False,\n",
       "         use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_MultinomialNB = Pipeline([('vect', CountVectorizer(analyzer=split_into_lemmas, ngram_range=(2,4), stop_words='english',lowercase=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf_MultinomialNB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2>Making pipeline of SVC<h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer=<function split_into_lemmas at 0x11bd320c8>,\n",
       "        binary=False, decode_error=u'strict', dtype=<type 'numpy.int64'>,\n",
       "        encoding=u'utf-8', input=u'content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(2, 4), preprocess...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_SVC = Pipeline([('vect', CountVectorizer(analyzer=split_into_lemmas, ngram_range=(2,4), stop_words='english',lowercase=True)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SVC(kernel='linear')),\n",
    "])\n",
    "text_clf_SVC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Learning of data is done by using fit method. Now, our model is ready to predict on the test data.\n",
    "Let's start prediction.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_SVC = text_clf_SVC.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_MultinomialNB = text_clf_MultinomialNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_LogisticRegression = text_clf_LogisticRegression.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_SGDClassifier = text_clf_SGDClassifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.8.11-cp38-cp38-win_amd64.whl (49.0 MB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in d:\\anaconda\\lib\\site-packages (from mediapipe) (21.2.0)\n",
      "Requirement already satisfied: absl-py in d:\\anaconda\\lib\\site-packages (from mediapipe) (1.0.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in d:\\anaconda\\lib\\site-packages (from mediapipe) (3.20.1)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from mediapipe) (1.20.3)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\lib\\site-packages (from mediapipe) (3.4.2)\n",
      "Collecting opencv-contrib-python\n",
      "  Downloading opencv_contrib_python-4.6.0.66-cp36-abi3-win_amd64.whl (42.5 MB)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from absl-py->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (8.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (2.4.7)\n",
      "Installing collected packages: opencv-contrib-python, mediapipe\n",
      "Successfully installed mediapipe-0.8.11 opencv-contrib-python-4.6.0.66\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "mpPose = mp.solutions.pose\n",
    "pose = mpPose.Pose()\n",
    "mpDraw = mp.solutions.drawing_utils # For drawing keypoints\n",
    "points = mpPose.PoseLandmark # Landmarks\n",
    "path = \"D:/downloads/DATASET/TRAIN/plank\" # enter dataset path\n",
    "data = []\n",
    "for p in points:\n",
    "        x = str(p)[13:]\n",
    "        data.append(x + \"_x\")\n",
    "        data.append(x + \"_y\")\n",
    "        data.append(x + \"_z\")\n",
    "        data.append(x + \"_vis\")\n",
    "data = pd.DataFrame(columns = data) # Empty dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for img in os.listdir(path):\n",
    "\n",
    "        temp = []\n",
    "\n",
    "        img = cv2.imread(path + \"/\" + img)\n",
    "\n",
    "        imageWidth, imageHeight = img.shape[:2]\n",
    "\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        blackie = np.zeros(img.shape) # Blank image\n",
    "\n",
    "        results = pose.process(imgRGB)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "\n",
    "                # mpDraw.draw_landmarks(img, results.pose_landmarks, mpPose.POSE_CONNECTIONS) #draw landmarks on image\n",
    "\n",
    "                mpDraw.draw_landmarks(blackie, results.pose_landmarks, mpPose.POSE_CONNECTIONS) # draw landmarks on blackie\n",
    "\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                for i,j in zip(points,landmarks):\n",
    "\n",
    "                        temp = temp + [j.x, j.y, j.z, j.visibility]\n",
    "\n",
    "                data.loc[count] = temp\n",
    "\n",
    "                count +=1\n",
    "\n",
    "        #cv2.imshow(\"Image\", img)\n",
    "\n",
    "        #cv2.imshow(\"blackie\",blackie)\n",
    "\n",
    "        #cv2.waitKey(100)\n",
    "\n",
    "data.to_csv(\"dataset3.csv\") # save the data as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
